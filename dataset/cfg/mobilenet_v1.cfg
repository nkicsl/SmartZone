[net]
# Training
# batch=128
# subdivisions=4

# Testing
batch=1
subdivisions=1

height=256
width=256
channels=3
min_crop=128
max_crop=448

burn_in=1000
learning_rate=0.1
policy=poly
power=4
max_batches=800000
momentum=0.9
decay=0.0005

angle=7
hue=.1
saturation=.75
exposure=.75
aspect=.75

[convolutional]
batch_normalize=1
filters=32
size=3
stride=2
pad=1
activation=leaky

#1th 3x3
[depthwise_convolutional]
batch_normalize=1
#filters=32
size=3
stride=1
pad=1
#group=32
activation=leaky

#1th 1x1
[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=0
activation=leaky

#2th 3x3
[depthwise_convolutional]
batch_normalize=1
#filters=64
size=3
stride=2
pad=1
#group=64
activation=leaky

#2th 1x1
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=0
activation=leaky

#3th 3x3
[depthwise_convolutional]
batch_normalize=1
#filters=128
size=3
stride=1
pad=1
#group=128
activation=leaky

#3th 1x1
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=0
activation=leaky

#4th 3x3
[depthwise_convolutional]
batch_normalize=1
#filters=128
size=3
stride=2
pad=1
#group=128
activation=leaky

#4th 1x1                                              
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=0
activation=leaky

#5th 3x3
[depthwise_convolutional]
batch_normalize=1
#filters=256
size=3
stride=1
pad=1
#group=256
activation=leaky

#5th 1x1
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=0
activation=leaky

#6th 3x3
[depthwise_convolutional]
batch_normalize=1
#filters=256
size=3
stride=2
pad=1
#group=256
activation=leaky

#6th 1x1
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=0
activation=leaky

#7th 3x3
[depthwise_convolutional]
batch_normalize=1
#filters=512
size=3
stride=1
pad=1
#group=512
activation=leaky

#7th 1x1
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=0
activation=leaky

#8th 3x3
[depthwise_convolutional]
batch_normalize=1
#filters=512
size=3
stride=1
pad=1
#group=512
activation=leaky

#8th 1x1
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=0
activation=leaky


#9th 3x3
[depthwise_convolutional]
batch_normalize=1
#filters=512
size=3
stride=1
pad=1
#group=512
activation=leaky

#9th 1x1
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=0
activation=leaky

#10th 3x3
[depthwise_convolutional]
batch_normalize=1
#filters=512
size=3
stride=1
pad=1
#group=512
activation=leaky

#10th 1x1
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=0
activation=leaky

#11th 3x3
[depthwise_convolutional]
batch_normalize=1
#filters=512
size=3
stride=1
pad=1
#group=512
activation=leaky

#11th 1x1
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=0
activation=leaky

#12th 3x3
[depthwise_convolutional]
batch_normalize=1
#filters=512
size=3
stride=2
pad=1
#group=512
activation=leaky

#12th 1x1
[convolutional]
batch_normalize=1
filters=1024
size=1
stride=1
pad=0
activation=leaky


#13th 3x3
[depthwise_convolutional]
batch_normalize=1
#filters=1024
size=3
stride=1
pad=1
#group=1024
activation=leaky

#13th 1x1
[convolutional]
batch_normalize=1
filters=1024
size=1
stride=1
pad=0
activation=leaky

[avgpool]

[convolutional]
filters=1000
size=1
stride=1
activation=linear

[softmax]
groups=1

